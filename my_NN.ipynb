{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    def __init__(self,sizes,beta=0.9,gamma=0.99,alpha=0.08):\n",
    "        self.Di = sizes[0]\n",
    "        self.Do = sizes[-1]\n",
    "        self.Dh = sizes[1:-1]\n",
    "        self.K = len(sizes)-1\n",
    "        self.beta = []\n",
    "        self.Omega = []\n",
    "        self.m = []\n",
    "        self.v = []\n",
    "        self.m_decay = beta\n",
    "        self.v_decay = gamma\n",
    "        self.alpha = alpha\n",
    "        self.t = 0\n",
    "        for index in range(self.K):\n",
    "            sigma = np.sqrt(4/(sizes[index]+sizes[index+1]))\n",
    "            self.beta.append(np.random.normal(size=(sizes[index+1],1)) * sigma)\n",
    "            self.Omega.append(np.random.normal(size=(sizes[index+1],sizes[index])) * sigma)\n",
    "            self.m.append([])\n",
    "            self.v.append([])\n",
    "\n",
    "            self.m[index].append(np.zeros((sizes[index+1],1)))\n",
    "            self.v[index].append(np.zeros((sizes[index+1],1)))\n",
    "            self.m[index].append(np.zeros((sizes[index+1],sizes[index])))\n",
    "            self.v[index].append(np.zeros((sizes[index+1],sizes[index])))\n",
    "    \n",
    "    def run_all(self,x):\n",
    "        values = [x]\n",
    "        for layer in range(self.K):\n",
    "            pre_act = np.matmul(self.Omega[layer],values[-1]) + self.beta[layer]\n",
    "            values.append(self.ReLU(pre_act))\n",
    "        return values\n",
    "    \n",
    "    def compute_gradient(self,x,y):\n",
    "        values = self.run_all(x)\n",
    "        grad_beta = [None] * self.K\n",
    "        grad_Omega = [None] * self.K\n",
    "        grad_beta[-1] = values[-1]-y\n",
    "\n",
    "        for layer in range(self.K-1,-1,-1):\n",
    "            if layer != self.K-1:\n",
    "                grad_activation = values[layer+1].astype(bool)*1.1 - 0.1\n",
    "                OmegaT = np.transpose(self.Omega[layer+1])\n",
    "                grad_beta[layer] = (np.matmul(OmegaT,grad_beta[layer+1])) * grad_activation\n",
    "            hT = np.transpose(values[layer],axes=[0,2,1])\n",
    "            grad_Omega[layer] = np.matmul(grad_beta[layer],hT)\n",
    "        \n",
    "        for layer in range(self.K):\n",
    "            grad_beta[layer] = sum(grad_beta[layer])/x.shape[0]\n",
    "            grad_Omega[layer] = sum(grad_Omega[layer])/x.shape[0]\n",
    "\n",
    "        return grad_beta,grad_Omega\n",
    "    \n",
    "    def update_weights(self,grad_beta,grad_Omega):\n",
    "        self.t += 1\n",
    "        for layer in range(self.K):\n",
    "            self.m[layer][0] *= self.m_decay\n",
    "            self.m[layer][0] += grad_beta[layer] * (1-self.m_decay)\n",
    "            self.m[layer][1] *= self.m_decay\n",
    "            self.m[layer][1] += grad_Omega[layer] * (1-self.m_decay)\n",
    "\n",
    "            self.v[layer][0] *= self.v_decay\n",
    "            self.v[layer][0] += grad_beta[layer]**2 * (1-self.v_decay)\n",
    "            self.v[layer][1] *= self.v_decay\n",
    "            self.v[layer][1] += grad_Omega[layer]**2 * (1-self.v_decay)\n",
    "\n",
    "            factor = (1-self.m_decay**self.t)/(1-self.v_decay**self.t)\n",
    "\n",
    "            delta_beta = -1 * self.alpha * self.m[layer][0] / (np.sqrt(self.v[layer][0]) + 1e-12)\n",
    "            delta_beta /= factor\n",
    "            self.beta[layer] += delta_beta \n",
    "\n",
    "            delta_Omega = -1 * self.alpha * self.m[layer][1] / (np.sqrt(self.v[layer][1]) + 1e-12)\n",
    "            delta_Omega /= factor\n",
    "            self.Omega[layer] += delta_Omega \n",
    "    \n",
    "    def run(self,x):\n",
    "        return self.run_all(x)[-1]\n",
    "    \n",
    "    def train_epoch(self,data,batch_num):\n",
    "        batch_indices = np.linspace(0,len(data[0]),batch_num+1).astype(int)\n",
    "        order = np.random.permutation(len(data[0]))\n",
    "        for num in range(batch_num):\n",
    "            xs = data[0][order[batch_indices[num]:batch_indices[num+1]]]\n",
    "            ys = data[1][order[batch_indices[num]:batch_indices[num+1]]]\n",
    "\n",
    "            grad_beta,grad_Omega = self.compute_gradient(xs,ys)\n",
    "\n",
    "            self.update_weights(grad_beta,grad_Omega)\n",
    "    \n",
    "    def train(self,data,epochs=100,batch_num=1):\n",
    "        for epoch in range(epochs):\n",
    "            self.train_epoch(data,batch_num)\n",
    "            print(f\"Loss: {self.loss(data)}\")\n",
    "        print(f\"Final loss: {self.loss(data)}\")\n",
    "\n",
    "    def ReLU(self,arr):\n",
    "        pos = arr.clip(min=0.0)\n",
    "        neg = arr.clip(max=0.0)*0.1\n",
    "        return pos+neg\n",
    "    \n",
    "    def loss(self,data):\n",
    "        y_pred = self.run(data[0])\n",
    "        return np.sum(data[1]-y_pred)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.632690853372921\n",
      "Loss: 2.1432222426634726\n",
      "Loss: 1.7866356015478255\n",
      "Loss: 2.935068051267336\n",
      "Loss: 8.564830774301646\n",
      "Loss: 1.4193572727196666\n",
      "Loss: 4.5055553263188886\n",
      "Loss: 0.6435312056377346\n",
      "Loss: 17.03876516189159\n",
      "Loss: 4.186206081300191\n",
      "Loss: 3.076721184490269\n",
      "Loss: 5.91293590808331\n",
      "Loss: 0.6792814577410462\n",
      "Loss: 7.87645638153392\n",
      "Loss: 1.911209945936933\n",
      "Loss: 3.5853523372224947\n",
      "Loss: 3.3819698043496205\n",
      "Loss: 0.5516474369752723\n",
      "Loss: 1.5334551543365311\n",
      "Loss: 0.25612938318770995\n",
      "Loss: 3.2871034320467443\n",
      "Loss: 0.5811118898445617\n",
      "Loss: 0.8853890295585304\n",
      "Loss: 2.0085461402828937\n",
      "Loss: 0.22562381519607708\n",
      "Loss: 0.33276970896137414\n",
      "Loss: 0.2254448076083678\n",
      "Loss: 0.19522109397052237\n",
      "Loss: 0.7572368130967829\n",
      "Loss: 0.07086497696861475\n",
      "Loss: 0.8004882007393531\n",
      "Loss: 0.9945011009247355\n",
      "Loss: 0.03373105018445637\n",
      "Loss: 0.8354500612164436\n",
      "Loss: 0.19560653798735106\n",
      "Loss: 0.12523205946142524\n",
      "Loss: 0.03581257295804273\n",
      "Loss: 0.34863796988207224\n",
      "Loss: 0.20009317953323702\n",
      "Loss: 0.3895953081163221\n",
      "Loss: 1.0244083320965993\n",
      "Loss: 0.014826030215308929\n",
      "Loss: 0.6370290186726446\n",
      "Loss: 0.017107025979049447\n",
      "Loss: 0.38388118365232216\n",
      "Loss: 0.0019294071205262582\n",
      "Loss: 0.7961028976702519\n",
      "Loss: 0.19780149337252936\n",
      "Loss: 0.5058146376900392\n",
      "Loss: 0.48595395411379133\n",
      "Loss: 0.15639431104314577\n",
      "Loss: 0.3524555574040593\n",
      "Loss: 0.06779448279888328\n",
      "Loss: 0.3765426599960376\n",
      "Loss: 0.06883441901959214\n",
      "Loss: 0.6100108357090152\n",
      "Loss: 0.05101056927042256\n",
      "Loss: 0.34472714883678496\n",
      "Loss: 0.063261975696102\n",
      "Loss: 0.15400920016083447\n",
      "Loss: 0.048524269223541826\n",
      "Loss: 0.18594434977094038\n",
      "Loss: 0.1992356397607538\n",
      "Loss: 0.041508297466689686\n",
      "Loss: 0.229362498370251\n",
      "Loss: 0.01089977229041763\n",
      "Loss: 0.12052846196438571\n",
      "Loss: 0.01271197152713679\n",
      "Loss: 0.09123386004185921\n",
      "Loss: 0.037094856403863036\n",
      "Loss: 0.08684092089850998\n",
      "Loss: 0.09583222051889223\n",
      "Loss: 0.030637145137263476\n",
      "Loss: 0.0990533564751935\n",
      "Loss: 0.00020862880656203524\n",
      "Loss: 0.04068636713648534\n",
      "Loss: 0.0006022104435297922\n",
      "Loss: 0.036813965748219823\n",
      "Loss: 0.0064423044358725675\n",
      "Loss: 0.048484700770282996\n",
      "Loss: 0.025907268968991183\n",
      "Loss: 0.025821398105835707\n",
      "Loss: 0.024963390010654098\n",
      "Loss: 0.008851956580821026\n",
      "Loss: 0.012756427164289055\n",
      "Loss: 0.003769791153845741\n",
      "Loss: 0.009940060276388624\n",
      "Loss: 0.003561438634896607\n",
      "Loss: 0.022181381070665234\n",
      "Loss: 0.0023005360296206863\n",
      "Loss: 0.022721982613446025\n",
      "Loss: 0.0005560875226348936\n",
      "Loss: 0.0073854802253321295\n",
      "Loss: 0.0007985410519578977\n",
      "Loss: 0.005346089622627515\n",
      "Loss: 0.0011976629516040331\n",
      "Loss: 0.011478551116531193\n",
      "Loss: 0.002088509629164362\n",
      "Loss: 0.008526407540369794\n",
      "Loss: 0.0010946139284276472\n",
      "Final loss: 0.0010946139284276472\n"
     ]
    }
   ],
   "source": [
    "myNN = DNN(sizes=[784,300,20,10])\n",
    "xs = np.random.normal(size=(10,myNN.Di,1))\n",
    "ys = np.random.normal(size=(10,myNN.Do,1))\n",
    "data = [xs,ys]\n",
    "myNN.train(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
