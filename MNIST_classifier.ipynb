{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    def __init__(self,sizes,beta=0.9,gamma=0.99,alpha=0.08):\n",
    "        self.Di = sizes[0]\n",
    "        self.Do = sizes[-1]\n",
    "        self.Dh = sizes[1:-1]\n",
    "        self.K = len(sizes)-1\n",
    "        self.beta = []\n",
    "        self.Omega = []\n",
    "        self.m = []\n",
    "        self.v = []\n",
    "        self.m_decay = beta\n",
    "        self.v_decay = gamma\n",
    "        self.alpha = alpha\n",
    "        self.t = 0\n",
    "        for index in range(self.K):\n",
    "            sigma = np.sqrt(4/(sizes[index]+sizes[index+1]))\n",
    "            self.beta.append(np.random.normal(size=(sizes[index+1],1)) * sigma)\n",
    "            self.Omega.append(np.random.normal(size=(sizes[index+1],sizes[index])) * sigma)\n",
    "            self.m.append([])\n",
    "            self.v.append([])\n",
    "\n",
    "            self.m[index].append(np.zeros((sizes[index+1],1)))\n",
    "            self.v[index].append(np.zeros((sizes[index+1],1)))\n",
    "            self.m[index].append(np.zeros((sizes[index+1],sizes[index])))\n",
    "            self.v[index].append(np.zeros((sizes[index+1],sizes[index])))\n",
    "    \n",
    "    def run_all(self,x):\n",
    "        values = [x]\n",
    "        for layer in range(self.K):\n",
    "            pre_act = np.matmul(self.Omega[layer],values[-1]) + self.beta[layer]\n",
    "            values.append(self.ReLU(pre_act))\n",
    "        return values\n",
    "    \n",
    "    def Ps(self,values):\n",
    "        exp_outs = np.exp(values)\n",
    "        exp_sums = np.sum(exp_outs,axis=1)\n",
    "        exp_sums=exp_sums.reshape(exp_sums.shape+(1,))\n",
    "        Ps = exp_outs / exp_sums\n",
    "        return Ps\n",
    "    \n",
    "    def compute_gradient(self,x,y):\n",
    "        values = self.run_all(x)\n",
    "        grad_beta = [None] * self.K\n",
    "        grad_Omega = [None] * self.K\n",
    "        grad_beta[-1] = self.Ps(values[-1]) - y\n",
    "\n",
    "        for layer in range(self.K-1,-1,-1):\n",
    "            if layer != self.K-1:\n",
    "                grad_activation = values[layer+1].astype(bool)*1.1 - 0.1\n",
    "                OmegaT = np.transpose(self.Omega[layer+1])\n",
    "                grad_beta[layer] = (np.matmul(OmegaT,grad_beta[layer+1])) * grad_activation\n",
    "            hT = np.transpose(values[layer],axes=[0,2,1])\n",
    "            grad_Omega[layer] = np.matmul(grad_beta[layer],hT)\n",
    "        \n",
    "        for layer in range(self.K):\n",
    "            grad_beta[layer] = sum(grad_beta[layer])/x.shape[0]\n",
    "            grad_Omega[layer] = sum(grad_Omega[layer])/x.shape[0]\n",
    "\n",
    "        return grad_beta,grad_Omega\n",
    "    \n",
    "    def update_weights(self,grad_beta,grad_Omega):\n",
    "        self.t += 1\n",
    "        for layer in range(self.K):\n",
    "            self.m[layer][0] *= self.m_decay\n",
    "            self.m[layer][0] += grad_beta[layer] * (1-self.m_decay)\n",
    "            self.m[layer][1] *= self.m_decay\n",
    "            self.m[layer][1] += grad_Omega[layer] * (1-self.m_decay)\n",
    "\n",
    "            self.v[layer][0] *= self.v_decay\n",
    "            self.v[layer][0] += grad_beta[layer]**2 * (1-self.v_decay)\n",
    "            self.v[layer][1] *= self.v_decay\n",
    "            self.v[layer][1] += grad_Omega[layer]**2 * (1-self.v_decay)\n",
    "\n",
    "            factor = (1-self.m_decay**self.t)/(1-self.v_decay**self.t)\n",
    "\n",
    "            delta_beta = -1 * self.alpha * self.m[layer][0] / (np.sqrt(self.v[layer][0]) + 1e-12)\n",
    "            delta_beta /= factor\n",
    "            self.beta[layer] += delta_beta \n",
    "\n",
    "            delta_Omega = -1 * self.alpha * self.m[layer][1] / (np.sqrt(self.v[layer][1]) + 1e-12)\n",
    "            delta_Omega /= factor\n",
    "            self.Omega[layer] += delta_Omega \n",
    "    \n",
    "    def run(self,x):\n",
    "        return self.run_all(x)[-1]\n",
    "    \n",
    "    def train_epoch(self,data,batch_size):\n",
    "        batch_num = int(data[0].shape[0]/batch_size)\n",
    "        batch_indices = np.linspace(0,len(data[0]),batch_num+1).astype(int)\n",
    "        order = np.random.permutation(len(data[0]))\n",
    "        for num in range(batch_num):\n",
    "            xs = data[0][order[batch_indices[num]:batch_indices[num+1]]]\n",
    "            ys = data[1][order[batch_indices[num]:batch_indices[num+1]]]\n",
    "\n",
    "            grad_beta,grad_Omega = self.compute_gradient(xs,ys)\n",
    "\n",
    "            self.update_weights(grad_beta,grad_Omega)\n",
    "            print(f\"Batch Loss: {self.loss([xs,ys])}\")\n",
    "    \n",
    "    def train(self,data,epochs=100,batch_size=1000):\n",
    "        ys = np.zeros((data[0].shape[0],self.Do,1))\n",
    "        for ind, ans in enumerate(data[1]):\n",
    "            ys[ind][int(ans[0])] += 1\n",
    "        real_data = [data[0],ys]\n",
    "        for epoch in range(epochs):\n",
    "            self.train_epoch(real_data,batch_size)\n",
    "            print(f\"Epoch Loss: {self.loss(real_data)}\")\n",
    "        print(f\"Final loss: {self.loss(real_data)}\")\n",
    "\n",
    "    def ReLU(self,arr):\n",
    "        pos = arr.clip(min=0.0)\n",
    "        neg = arr.clip(max=0.0)*0.1\n",
    "        return pos+neg\n",
    "    \n",
    "    def loss(self,data):\n",
    "        values = self.run(data[0])\n",
    "        derivs = self.Ps(values)\n",
    "        derivs *= data[1]\n",
    "        Ps = np.sum(derivs,axis=1)\n",
    "        return -1 * np.sum(np.log(Ps))\n",
    "    \n",
    "    def solve(self,x):\n",
    "        values = self.run(x)\n",
    "        derivs = self.Ps(values)\n",
    "        locs = np.argmax(derivs,axis=1)\n",
    "        return locs\n",
    "    \n",
    "    def rate(self,data):\n",
    "        diffs = (self.solve(data[0])-data[1]).astype(bool)\n",
    "        incorrect = int(np.sum(diffs))\n",
    "        return 1-incorrect/data[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNN = DNN(sizes=[10,100,70,40,10])\n",
    "size = 10\n",
    "xs = np.random.normal(size=(size,myNN.Di,1))\n",
    "ys = np.zeros((size,myNN.Do,1))\n",
    "for ind in range(size):\n",
    "    ys[ind][randint(0,size-1)] += 1\n",
    "data = [xs,ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Epoch Loss: 30.682426732083414\n",
      "Final loss: 30.682426732083414\n"
     ]
    }
   ],
   "source": [
    "myNN.train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(\"MNIST/train_images.npy\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(\"MNIST/train_labels.npy\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.load(\"MNIST/train_images.npy\")\n",
    "xs = xs.reshape(xs.shape+(1,))/256\n",
    "ys = np.load(\"MNIST/train_labels.npy\")\n",
    "cutoff = int(0.9 * xs.shape[0])\n",
    "train_xs = xs[:cutoff]\n",
    "test_xs = xs[cutoff:]\n",
    "train_ys = ys[:cutoff]\n",
    "test_ys = ys[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNN = DNN(sizes=[784,300,100,30,10],alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 2441.997399884678\n",
      "Batch Loss: 2163.1896181961065\n",
      "Batch Loss: 2015.7149861333098\n",
      "Batch Loss: 1696.8917697224374\n",
      "Batch Loss: 1341.0624366026095\n",
      "Batch Loss: 1136.5765817953115\n",
      "Batch Loss: 1043.7375931370198\n",
      "Batch Loss: 1070.8948017907728\n",
      "Batch Loss: 840.6059058681442\n",
      "Batch Loss: 712.4969300067733\n",
      "Batch Loss: 845.3011360693779\n",
      "Batch Loss: 801.9219619821274\n",
      "Batch Loss: 759.9263026102903\n",
      "Batch Loss: 700.1423541311367\n",
      "Batch Loss: 722.6635718027953\n",
      "Batch Loss: 718.6291318426927\n",
      "Batch Loss: 730.6803233464307\n",
      "Batch Loss: 530.5902894270587\n",
      "Batch Loss: 640.3772276463962\n",
      "Batch Loss: 567.030863499988\n",
      "Batch Loss: 618.7203340193626\n",
      "Batch Loss: 667.1295003717858\n",
      "Batch Loss: 536.7115780880262\n",
      "Batch Loss: 475.4411847505417\n",
      "Batch Loss: 495.06167873563015\n",
      "Batch Loss: 599.6903214963697\n",
      "Batch Loss: 455.2520602609834\n",
      "Batch Loss: 586.8535405314512\n",
      "Batch Loss: 526.9107600972164\n",
      "Batch Loss: 450.13181049080083\n",
      "Batch Loss: 450.23083506486444\n",
      "Batch Loss: 566.9771588354249\n",
      "Batch Loss: 451.87477687679535\n",
      "Batch Loss: 406.26770854303334\n",
      "Batch Loss: 389.2977107294374\n",
      "Batch Loss: 443.75437189597903\n",
      "Batch Loss: 458.740019497712\n",
      "Batch Loss: 467.0274749480134\n",
      "Batch Loss: 483.6152662373569\n",
      "Batch Loss: 381.1669460109312\n",
      "Batch Loss: 409.9626142743482\n",
      "Batch Loss: 408.82684457024243\n",
      "Batch Loss: 347.4990309316812\n",
      "Batch Loss: 402.6649507122271\n",
      "Batch Loss: 357.63147552035076\n",
      "Batch Loss: 388.68414852630303\n",
      "Batch Loss: 315.74465323103124\n",
      "Batch Loss: 326.4708988792423\n",
      "Batch Loss: 272.3974421245408\n",
      "Batch Loss: 383.5587144303209\n",
      "Batch Loss: 332.43743277941036\n",
      "Batch Loss: 414.57734910460874\n",
      "Batch Loss: 312.8629324491389\n",
      "Batch Loss: 310.62258133019463\n",
      "Epoch Loss: 17977.538984376886\n",
      "Final loss: 17977.538984376886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9241666666666667"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myNN.train([train_xs,train_ys],epochs=1,batch_size=1000)\n",
    "myNN.rate([test_xs,test_ys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 296.55815958815646\n",
      "Batch Loss: 272.8665822111809\n",
      "Batch Loss: 270.9523535924417\n",
      "Batch Loss: 346.89144819184406\n",
      "Batch Loss: 369.59329991936346\n",
      "Batch Loss: 322.9594643845679\n",
      "Batch Loss: 364.0716983077774\n",
      "Batch Loss: 274.85030755839506\n",
      "Batch Loss: 356.9355273650043\n",
      "Batch Loss: 295.6208558662754\n",
      "Batch Loss: 275.16339093579677\n",
      "Batch Loss: 278.64109426166317\n",
      "Batch Loss: 368.7000353118458\n",
      "Batch Loss: 372.55725156759814\n",
      "Batch Loss: 359.4952763750249\n",
      "Batch Loss: 418.1689448007305\n",
      "Batch Loss: 455.2445959351807\n",
      "Batch Loss: 398.48329200063324\n",
      "Batch Loss: 330.3535155610849\n",
      "Batch Loss: 397.5001990921752\n",
      "Batch Loss: 369.34644571490935\n",
      "Batch Loss: 289.9711383660492\n",
      "Batch Loss: 402.4823289517569\n",
      "Batch Loss: 345.3819618168701\n",
      "Batch Loss: 355.90228595569135\n",
      "Batch Loss: 324.1352082550153\n",
      "Batch Loss: 385.654515723749\n",
      "Batch Loss: 309.39385638115914\n",
      "Batch Loss: 285.93566820588785\n",
      "Batch Loss: 331.1602944314314\n",
      "Batch Loss: 250.11193479456938\n",
      "Batch Loss: 322.5295852432842\n",
      "Batch Loss: 264.5357499108623\n",
      "Batch Loss: 352.6376625541631\n",
      "Batch Loss: 336.5187049557384\n",
      "Batch Loss: 341.97348085491006\n",
      "Batch Loss: 381.09253326442445\n",
      "Batch Loss: 315.0648907546996\n",
      "Batch Loss: 311.65028530139784\n",
      "Batch Loss: 314.4707604790995\n",
      "Batch Loss: 321.84625606980285\n",
      "Batch Loss: 319.0420382197078\n",
      "Batch Loss: 313.9713090770433\n",
      "Batch Loss: 343.6224747955166\n",
      "Batch Loss: 306.5859624761528\n",
      "Batch Loss: 255.5644863290355\n",
      "Batch Loss: 363.81172713406875\n",
      "Batch Loss: 295.54555949695424\n",
      "Batch Loss: 389.2911362870101\n",
      "Batch Loss: 372.8240026776906\n",
      "Batch Loss: 346.1782654857589\n",
      "Batch Loss: 355.497544909835\n",
      "Batch Loss: 320.19609338110115\n",
      "Batch Loss: 326.7215511170357\n",
      "Epoch Loss: 17687.557692521175\n",
      "Final loss: 17687.557692521175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.925"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myNN.train([train_xs,train_ys],epochs=1,batch_size=1000)\n",
    "myNN.rate([test_xs,test_ys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 892.6762079839318\n",
      "Batch Loss: 924.962325140333\n",
      "Batch Loss: 819.3594318079709\n",
      "Batch Loss: 891.2771453552114\n",
      "Batch Loss: 1005.2267528532267\n",
      "Batch Loss: 874.100208948423\n",
      "Batch Loss: 870.300870657104\n",
      "Batch Loss: 793.4438916272078\n",
      "Batch Loss: 938.8960349408045\n",
      "Batch Loss: 839.1918762626967\n",
      "Batch Loss: 830.2338820028468\n",
      "Batch Loss: 939.825723281219\n",
      "Batch Loss: 930.2313411493872\n",
      "Batch Loss: 752.7885522767273\n",
      "Batch Loss: 888.2766201271098\n",
      "Batch Loss: 832.899818483216\n",
      "Batch Loss: 741.2845273166188\n",
      "Batch Loss: 838.15847371281\n",
      "Epoch Loss: 14874.36923977339\n",
      "Final loss: 14874.36923977339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9355"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myNN.train([train_xs,train_ys],epochs=1,batch_size=3000)\n",
    "myNN.rate([test_xs,test_ys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 729.7229384450721\n",
      "Batch Loss: 797.253528540409\n",
      "Batch Loss: 718.0089705489268\n",
      "Batch Loss: 771.7028263111329\n",
      "Batch Loss: 683.4828912547499\n",
      "Batch Loss: 778.2078762825831\n",
      "Batch Loss: 766.0696978923578\n",
      "Batch Loss: 803.4939906366685\n",
      "Batch Loss: 827.1552022077354\n",
      "Batch Loss: 660.5489696931106\n",
      "Batch Loss: 854.048846787631\n",
      "Batch Loss: 657.1508016504215\n",
      "Batch Loss: 850.3908203793004\n",
      "Batch Loss: 861.664724604472\n",
      "Batch Loss: 773.9153335126291\n",
      "Batch Loss: 765.8513938189956\n",
      "Batch Loss: 886.9095631897976\n",
      "Batch Loss: 707.0959811895598\n",
      "Epoch Loss: 12290.473802190538\n",
      "Batch Loss: 627.0486513195572\n",
      "Batch Loss: 701.0821367869951\n",
      "Batch Loss: 761.4707311820923\n",
      "Batch Loss: 604.2704342716214\n",
      "Batch Loss: 713.2553650168381\n",
      "Batch Loss: 702.0061221025148\n",
      "Batch Loss: 600.5082275995246\n",
      "Batch Loss: 680.9170613739848\n",
      "Batch Loss: 648.9715711776535\n",
      "Batch Loss: 665.4081316381216\n",
      "Batch Loss: 631.3112648688672\n",
      "Batch Loss: 611.0372495976967\n",
      "Batch Loss: 666.63614470435\n",
      "Batch Loss: 639.2362969978609\n",
      "Batch Loss: 638.023231316291\n",
      "Batch Loss: 652.6325942056544\n",
      "Batch Loss: 764.4109971014034\n",
      "Batch Loss: 573.3520464717932\n",
      "Epoch Loss: 12096.718476954418\n",
      "Batch Loss: 648.3290411301517\n",
      "Batch Loss: 633.5214293936122\n",
      "Batch Loss: 741.6223409127851\n",
      "Batch Loss: 703.6439469378831\n",
      "Batch Loss: 694.7496228195913\n",
      "Batch Loss: 600.0003591414553\n",
      "Batch Loss: 646.5689959414408\n",
      "Batch Loss: 622.6548010812155\n",
      "Batch Loss: 807.3411419323432\n",
      "Batch Loss: 689.2618614017006\n",
      "Batch Loss: 588.6628963586975\n",
      "Batch Loss: 708.7403867876459\n",
      "Batch Loss: 673.3110323174085\n",
      "Batch Loss: 681.7081980720457\n",
      "Batch Loss: 712.662012976782\n",
      "Batch Loss: 595.4402098172469\n",
      "Batch Loss: 740.1831267333308\n",
      "Batch Loss: 707.7700502963155\n",
      "Epoch Loss: 12523.16417462622\n",
      "Batch Loss: 688.6118714878728\n",
      "Batch Loss: 702.4130511047615\n",
      "Batch Loss: 907.9678741650343\n",
      "Batch Loss: 950.0457401594856\n",
      "Batch Loss: 623.1378298662803\n",
      "Batch Loss: 913.1267349395345\n",
      "Batch Loss: 749.1714464929785\n",
      "Batch Loss: 775.4964398141515\n",
      "Batch Loss: 736.6464364892342\n",
      "Batch Loss: 772.1565922643334\n",
      "Batch Loss: 620.004172619567\n",
      "Batch Loss: 934.7026094531797\n",
      "Batch Loss: 778.8836324120289\n",
      "Batch Loss: 681.6572180274162\n",
      "Batch Loss: 811.6457065077482\n",
      "Batch Loss: 725.1427200331673\n",
      "Batch Loss: 751.8590695827637\n",
      "Batch Loss: 778.7660654124409\n",
      "Epoch Loss: 13711.018336142513\n",
      "Batch Loss: 665.4325309311848\n",
      "Batch Loss: 770.2962775503603\n",
      "Batch Loss: 632.7238271301944\n",
      "Batch Loss: 717.6815322420421\n",
      "Batch Loss: 748.7847836968197\n",
      "Batch Loss: 686.3010492521075\n",
      "Batch Loss: 837.9279782200342\n",
      "Batch Loss: 684.8717849369514\n",
      "Batch Loss: 867.5047673652407\n",
      "Batch Loss: 707.4589425967117\n",
      "Batch Loss: 831.5480225984272\n",
      "Batch Loss: 762.9890168128791\n",
      "Batch Loss: 742.3193598275385\n",
      "Batch Loss: 712.6201123549683\n",
      "Batch Loss: 871.8058506237358\n",
      "Batch Loss: 963.926069476568\n",
      "Batch Loss: 928.8299084680684\n",
      "Batch Loss: 891.4263549369191\n",
      "Epoch Loss: 16304.348840688659\n",
      "Final loss: 16304.348840688659\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9145"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myNN.train([train_xs,train_ys],epochs=5,batch_size=3000)\n",
    "myNN.rate([test_xs,test_ys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9066481481481481"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myNN.rate([train_xs,train_ys])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
